# -*- coding: utf-8 -*-
"""ComputerVision-Ejercicios8_9

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OqZRUZGLqaqRgLXJA9Q-bWOT33jTxv4E
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
!pip install -q mediapy
import mediapy as media

"""# Ejercicio 8

En lo posible, realizar videos donde una clase de objeto difiera en número con otros objetos (sino, utilizar videos bajados de internet). Por ejemplo, al aire libre, un perro que se mueva entre personas en un parque. Una persona en bicicleta entre muchos autos en una calle de la ciudad. Luego, utilizar los distintos algoritmos presentados en teoría para detectar solo la clase minoritaria con su respectiva etiqueta. Indicando en cada caso la herramienta utilizada y comparando los niveles de confianza de detección logrados.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install pytube
# !apt update && apt install -y handbrake
# 
# from pytube import YouTube

yt = YouTube('https://www.youtube.com/watch?v=3ZXesHn7cAw')
video = yt.streams.filter(progressive=True, file_extension='mp4', res="720p").first()

video.download(filename='video_perro.mp4')
print("Descarga completada.")
!ffmpeg -y -i video_perro.mp4 -vf "scale=600:-1" -an -t 30 video_perro-final.mp4

# Commented out IPython magic to ensure Python compatibility.
# %pip install ultralytics
# Librería mediapy para mostrar videos
# %pip install -q mediapy
import ultralytics
ultralytics.checks()

from ultralytics import YOLO

# Load the pre-trained YOLOv8 model
model = YOLO('yolov8n')  # This will automatically download the model weights

# Función para detectar objetos en una imagen:
def detect_objects(new_image, prev_image, **kwargs):
    # Convertir la imagen a float32
    results = model(new_image)

    # Clases de interés para graficar
    classes = kwargs.get('classes', ['dog', 'person'])

    # Iteramos sobre los boung boxes obtenidos
    for box in results[0].boxes:
        # Extrayendo los datos del tensor
        x1, y1, x2, y2, confidence, cls = box.data[0]

        # Obteniendo el nombre de la clase
        class_name = model.names[int(cls)]

        # Parámetros opcionales del bounding box
        color = kwargs.get('color', (0, 255, 0))
        thickness = kwargs.get('thickness', 2)

        if class_name in classes:
            # Dibujar el rectángulo
            cv2.rectangle(new_image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)

            # Agregar el texto de la confianza
            confidence_text = f"{class_name}: {confidence:.2f}"
            cv2.putText(new_image, confidence_text, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    return new_image

# Función para procesar un video:
def video_processor(filename_in, filename_out, process_func, max_time=10, **kwargs):
    # Abrir el video de entrada para lectura
    with media.VideoReader(filename_in) as r:
        # Crear un archivo de video de salida
        with media.VideoWriter(filename_out, shape=r.shape, fps=r.fps, bps=r.bps) as w:
            count = 0  # Inicializar contador de fotogramas
            prev_image = None  # Inicializar la imagen previa

            # Iterar sobre cada imagen (fotograma) del video
            for image in r:
                new_image = media.to_uint8(image)  # Convertir la imagen a formato flotante

                # Comprobar si es la primera imagen
                if prev_image is None:
                    prev_image = new_image.copy()

                # Procesar la imagen utilizando la función dada
                processed_image = process_func(new_image, prev_image, **kwargs)

                # Añadir la imagen procesada al video de salida
                w.add_image(processed_image)

                # Actualizar la imagen previa
                prev_image = new_image.copy()

                # Incrementar el contador de fotogramas
                count += 1

                # Detener el proceso si se alcanza el tiempo máximo
                if count >= max_time * r.fps:
                    break

filename_in = 'video_perro-final.mp4'
filename_out = 'video_perro-final_yolo.mp4'
parameters = dict(classes=['dog', 'person'])

video_processor(filename_in, filename_out, detect_objects, 20, **parameters)

media.show_video(media.read_video(filename_out), fps=30)

"""El modelo detecta con facilidad al perro y a las dos personas que aparecen, excepto que reconoce como persona a una silla que aparece en el vídeo.

En el momento en el que la nena se cae, el algoritmo cambia de detectar una persona, a un perro, hasta que ella se levanta.

##Modelo MobileNetSSD
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !wget -O 'MobileNetSSD_deploy.prototxt' https://raw.githubusercontent.com/TheNsBhasin/DNN_Object_Detection/master/MobileNetSSD_deploy.prototxt.txt
# !wget -O 'MobileNetSSD_deploy.caffemodel' https://github.com/TheNsBhasin/DNN_Object_Detection/blob/master/MobileNetSSD_deploy.caffemodel?raw=true

from google.colab.patches import cv2_imshow

modelo = 'MobileNetSSD_deploy.caffemodel'
configuracion = 'MobileNetSSD_deploy.prototxt'
clases = ["background", "aeroplane", "bicycle", "bird", "boat",
          "bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
          "dog", "horse", "motorbike", "person", "pottedplant", "sheep",
          "sofa", "train", "tvmonitor"]
net = cv2.dnn.readNetFromCaffe(configuracion, modelo)


def obj_detc(imagen, prev_image, **kwargs):
  (h, w) = imagen.shape[:2]
  blob = cv2.dnn.blobFromImage(cv2.resize(imagen, (300, 300)), 0.007843, (300, 300), 127.5)

  net.setInput(blob)
  detecciones = net.forward()

  for i in np.arange(0, detecciones.shape[2]):
      confianza = detecciones[0, 0, i, 2]
      idx = int(detecciones[0, 0, i, 1])

      if confianza > 0.2 and idx == clases.index("dog"):
          box = detecciones[0, 0, i, 3:7] * np.array([w, h, w, h])
          (startX, startY, endX, endY) = box.astype("int")

          etiqueta = "{}: {:.2f}%".format(clases[idx], confianza * 100)
          cv2.rectangle(imagen, (startX, startY), (endX, endY), (0, 255, 0), 2)
          y = startY - 15 if startY - 15 > 15 else startY + 15
          cv2.putText(imagen, etiqueta, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

      return imagen

video_processor(filename_in, "video_perro-final_mobilenet.mp4", obj_detc, 20, **parameters)

media.show_video(media.read_video("video_perro-final_mobilenet.mp4"), fps=30)

"""Con este algoritmo, se observa que no se detectan correctamente a las personas y al perro

## Detección con DETR (Transformers)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers timm
# !wget "https://github.com/openmaptiles/fonts/raw/master/roboto/Roboto-Regular.ttf" -O "Roboto-Regular.ttf"

!pip install timm

from transformers import pipeline
from PIL import Image, ImageDraw, ImageFont
from IPython.display import display

font = ImageFont.truetype("Roboto-Regular.ttf", 40)
object_detector = pipeline("object-detection", model="facebook/detr-resnet-50")

def draw_bounding_box(im, score, label, xmin, ymin, xmax, ymax, index, num_boxes):
    im_with_rectangle = ImageDraw.Draw(im)
    im_with_rectangle.rounded_rectangle([xmin, ymin, xmax, ymax], outline="red", width=5, radius=10)

    im_with_rectangle.text((xmin + 10, ymin - 40), f"{label} ({score:.2f})", fill="white", font=font)

    return im


def detr(image, prev, **kwargs):
  im = Image.fromarray(image)
  bounding_boxes = object_detector(im)
  num_boxes = len(bounding_boxes)

  for index, bounding_box in enumerate(bounding_boxes):
      box = bounding_box['box']
      im = draw_bounding_box(im, bounding_box['score'], bounding_box['label'],
                              int(box['xmin']), int(box['ymin']), int(box['xmax']),
                              int(box['ymax']), index, num_boxes)

  return im

!huggingface-cli install facebook/detr-resnet-50

!pip show transformers

!pip install transformers --upgrade

model_name = "facebook/detr-resnet-50"

object_detector = pipeline("object-detection", model=model_name)

video_processor(filename_in, "video_perro_detr.mp4", detr, 20, **parameters)
media.show_video(media.read_video("video_perro_detr.mp4"), fps=30)

"""# Ejercicio 9

Tomar fotografías donde coexistan varios objetos en posiciones solapadas y no, en contextos de diferente complejidad. Luego, aplicar los algoritmos de segmentación propuestos y verificar los resultados de cada uno. Comentar qué diferencias observa.

###Instance Segmentation con Mask R-CNN (OpenCV)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !wget http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz
# !tar zxvf mask_rcnn_inception_v2_coco_2018_01_28.tar.gz
# !wget https://raw.githubusercontent.com/spmallick/learnopencv/master/Mask-RCNN/mscoco_labels.names -O mscoco_labels.names
# !wget https://raw.githubusercontent.com/spmallick/learnopencv/master/Mask-RCNN/colors.txt -O colors.txt
# !wget https://raw.githubusercontent.com/spmallick/learnopencv/master/Mask-RCNN/mask_rcnn_inception_v2_coco_2018_01_28.pbtxt -O mask_rcnn_inception_v2_coco_2018_01_28.pbtxt
# !wget https://raw.githubusercontent.com/matterport/Mask_RCNN/master/images/7933423348_c30bd9bd4e_z.jpg -O police_motorbike.jpg

import cv2 as cv
import time
import random
from matplotlib import pyplot as plt

# Umbral de confianza para las detecciones
confThreshold = 0.5
# Umbral para las máscaras de segmentación
maskThreshold = 0.3

# Cargar los nombres de las clases de MSCOCO
classesFile = "mscoco_labels.names"
classes = None
with open(classesFile, 'rt') as f:
    classes = f.read().rstrip('\n').split('\n')

# Cargar los colores para visualizar las detecciones
colorsFile = "colors.txt"
colors = []
with open(colorsFile, 'rt') as f:
    colorsStr = f.read().rstrip('\n').split('\n')
    for i in range(len(colorsStr)):
        rgb = colorsStr[i].split(' ')
        color = np.array([float(rgb[0]), float(rgb[1]), float(rgb[2])])
        colors.append(color)

# Archivos de configuración y pesos del modelo Mask R-CNN
textGraph = "./mask_rcnn_inception_v2_coco_2018_01_28.pbtxt"
modelWeights = "./mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb"

# Cargar la red
net = cv.dnn.readNetFromTensorflow(modelWeights, textGraph)
net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)
net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)

# Cargar la imagen y obtener sus dimensiones
image = cv.imread('peatonal.jpg')
(H, W) = image.shape[:2]

# Crear un blob de la imagen y realizar una pasada hacia adelante con Mask R-CNN
blob = cv.dnn.blobFromImage(image, swapRB=True, crop=False)
net.setInput(blob)
start = time.time()
(boxes, masks) = net.forward(["detection_out_final", "detection_masks"])
end = time.time()

# Información de tiempo y formas de las detecciones y máscaras
print("[INFO] Mask R-CNN tomó {:.6f} segundos en procesar la imagen".format(end - start))
print("[INFO] Forma de boxes: {}".format(boxes.shape))
print("[INFO] Forma de masks: {}".format(masks.shape))

output_image = image.copy()

for i in range(0, boxes.shape[2]):
    confidence = boxes[0, 0, i, 2]
    if confidence > confThreshold:
        classId = int(boxes[0, 0, i, 1])
        box = boxes[0, 0, i, 3:7] * np.array([W, H, W, H])
        (startX, startY, endX, endY) = box.astype("int")

        mask = masks[i, classId]
        mask = cv.resize(mask, (endX - startX, endY - startY))
        mask = (mask > maskThreshold).astype("uint8")

        roi = output_image[startY:endY, startX:endX]

        color = random.choice(colors)
        color_mask = np.zeros_like(roi)
        color_mask[mask == 1] = color

        roi = cv.addWeighted(roi, 1, color_mask, 0.4, 0)

        output_image[startY:endY, startX:endX] = roi

        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)

        classId = int(boxes[0, 0, i, 1])
        className = classes[classId]

        label = f"{className}: {confidence:.2f}"
        cv.rectangle(output_image, (startX, startY), (endX, endY), color, 2)
        cv.putText(output_image, label, (startX, startY - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)


# Configurar los subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 10))

# Mostrar la imagen original
axes[0].imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))
axes[0].set_title('Imagen Original')
axes[0].axis('off')  # Ocultar los ejes

# Mostrar la imagen segmentada
axes[1].imshow(cv.cvtColor(output_image, cv.COLOR_BGR2RGB))
axes[1].set_title('Imagen Segmentada R-CNN')
axes[1].axis('off')  # Ocultar los ejes

# Mostrar ambas imágenes
plt.show()

"""Este algoritmo detectó correctamente a la mayoría de objetos presentes en la imagen. Distingue entre personas y los paraguas, así que el nivel de confianza es alto.

Cabe destacar que también detectó el maniquí que se encuentra en la vidriera y lo identificó como una persona.

###Instance Segmentation con modelo PointRend Resnet50 (PixelLib)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pixellib torchvision
# # Instance segmentation model
# !wget "https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet50.pkl" -O pointrend_resnet50.pkl

# Importación de las bibliotecas necesarias
import pixellib
from pixellib.torchbackend.instance import instanceSegmentation
from IPython.display import Image

# Creación de una instancia del modelo de segmentación
ins = instanceSegmentation()

# Carga del modelo preentrenado
ins.load_model("pointrend_resnet50.pkl")

# Segmentación de la imagen y guardado de la imagen segmentada con las cajas delimitadoras
ins.segmentImage("peatonal.jpg", show_bboxes=True, output_image_name="peatonal_segmented.jpg")

# Muestra la imagen original
display(Image(filename="peatonal.jpg"))

# Muestra la imagen segmentada
display(Image(filename="peatonal_segmented.jpg"))

"""los resultados son similares con el algoritmo R-CNN, identifica correctamente a las personas y a los paraguas.

###Semantic Segmentation con modelo DeepLabV3 (Torchvision)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install torchvision
# import torch
# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)
# model.eval()

from PIL import Image
from torchvision import transforms
import torch

# Carga y procesa la imagen
input_image = Image.open("rosario.jpg").convert("RGB").convert("RGB")

preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0)  # crea un mini-batch como espera el modelo

# Mueve el input y el modelo a GPU para mayor velocidad si está disponible
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

with torch.no_grad():
    output = model(input_batch)['out'][0]
output_predictions = output.argmax(0)

# Crea una paleta de colores, seleccionando un color para cada clase
palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
colors = (colors % 255).numpy().astype("uint8")

# Genera la imagen de segmentación semántica
r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
r.putpalette(colors)

# Muestra la imagen original y la segmentada lado a lado
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)  # 1 fila, 2 columnas, primer subplot
plt.imshow(input_image)
plt.title('Imagen Original')
plt.axis('off')  # Oculta los ejes

plt.subplot(1, 2, 2)  # 1 fila, 2 columnas, segundo subplot
plt.imshow(r)
plt.title('Imagen Segmentada')
plt.axis('off')  # Oculta los ejes

plt.show()

"""Este algoritmo reconoce a las cuatro personas que están presentes en la imagen y los segmenta casi en su totalidad.

###Panoptic segmentation con modelo DETR ResNet50 (Transformers)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install timm

import io
import requests
from PIL import Image, ImageDraw, ImageFont
import torch
import itertools
import seaborn as sns
from transformers import DetrFeatureExtractor, DetrForSegmentation
from transformers.models.detr.feature_extraction_detr import rgb_to_id

# Cargar la imagen
image = cv2.imread("perros.jpg")

# Cargar el modelo y el extractor de características
feature_extractor = DetrFeatureExtractor.from_pretrained("facebook/detr-resnet-50-panoptic")
model = DetrForSegmentation.from_pretrained("facebook/detr-resnet-50-panoptic")

# Preparar la imagen para el modelo
inputs = feature_extractor(images=image, return_tensors="pt")

# Paso forward
outputs = model(**inputs)

# Postprocesamiento
processed_sizes = torch.as_tensor(inputs["pixel_values"].shape[-2:]).unsqueeze(0)
result = feature_extractor.post_process_panoptic(outputs, processed_sizes, threshold=0.85)[0]

# Convertir la segmentación a numpy
panoptic_seg = Image.open(io.BytesIO(result["png_string"]))
panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)
panoptic_seg_id = rgb_to_id(panoptic_seg)

# Preparar la paleta de colores
palette = itertools.cycle(sns.color_palette())

# Crear la imagen segmentada
segmented_image = Image.fromarray(np.zeros_like(panoptic_seg, dtype=np.uint8))
draw = ImageDraw.Draw(segmented_image)

# Añadimos un mapeo manual de los IDs de las categorías a los nombres, basado en las clases comunes de COCO
COCO_LABELS = {
    1: 'persona', 2: 'bicicleta', 3: 'coche', 4: 'motocicleta', 5: 'avión',
    6: 'autobús', 7: 'tren', 8: 'camión', 9: 'barco', 10: 'semáforo',
    11: 'hidrante', 13: 'señal de stop', 14: 'parquímetro', 15: 'banco', 16: 'pájaro',
    17: 'gato', 18: 'perro', 19: 'caballo', 20: 'oveja', 21: 'vaca',
    22: 'elefante', 23: 'oso', 24: 'cebra', 25: 'jirafa', 27: 'mochila',
    28: 'paraguas', 31: 'bolso de mano', 32: 'corbata', 33: 'maleta', 34: 'frisbee',
    36: 'tabla de snowboard', 37: 'pelota deportiva', 38: 'cometa', 39: 'bate de béisbol',
    40: 'guante de béisbol', 41: 'patineta', 42: 'tabla de surf', 43: 'raqueta de tenis',
    44: 'botella', 46: 'plato de vino', 47: 'taza', 48: 'tenedor', 49: 'cuchillo',
    50: 'cuchara', 51: 'tazón', 52: 'banana', 53: 'manzana', 54: 'sándwich',
    55: 'naranja', 56: 'brócoli', 57: 'zanahoria', 58: 'perrito caliente', 59: 'pizza',
    60: 'donut', 61: 'pastel', 62: 'silla', 63: 'sofá', 64: 'maceta', 65: 'cama',
    67: 'mesa de comedor', 70: 'inodoro', 72: 'TV', 73: 'computadora portátil', 74: 'ratón',
    75: 'control remoto', 76: 'teclado', 77: 'teléfono celular', 78: 'microondas',
    79: 'horno', 80: 'tostadora', 81: 'fregadero', 82: 'refrigerador', 84: 'libro',
    85: 'reloj', 86: 'florero', 87: 'tijeras', 88: 'oso de peluche', 89: 'secador de pelo',
    90: 'cepillo de dientes',
}

# Ajusta el bucle para dibujar segmentos y etiquetas correctamente
for segment_info in result["segments_info"]:
    class_id = segment_info["category_id"]
    class_name = COCO_LABELS.get(class_id, 'Desconocido')  # 'Desconocido' si el ID no está en el diccionario
    id = segment_info["id"]

    # Generar la máscara para este segmento específico
    mask = panoptic_seg_id == id
    color = np.array(next(palette)) * 255  # Convertir el color a un array de numpy adecuado

    # Convertir la máscara a una imagen de PIL para usarla como máscara en 'paste'
    mask_image = Image.fromarray((mask * 255).astype(np.uint8))

    # Crear una imagen del color del segmento que tenga las dimensiones correctas
    color_image = Image.new("RGB", segmented_image.size, color=tuple(color.astype(int)))

    # Pegar usando la máscara para aplicar solo este segmento
    segmented_image.paste(color_image, (0,0), mask=mask_image)

    # Dibujar el nombre de la clase en la posición inicial del segmento
    draw = ImageDraw.Draw(segmented_image)
    where = np.where(mask)
    if where[0].size > 0 and where[1].size > 0:
        x, y = np.min(where[1]), np.min(where[0])
        draw.text((x, y), class_name, fill='white')

# Mostrar la imagen original y la segmentada
plt.figure(figsize=(30, 15))
plt.subplot(1, 2, 1)
plt.imshow(image)
plt.title('Imagen Original')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(segmented_image)
plt.title('Imagen Segmentada con Etiquetas')
plt.axis('off')

plt.show()

"""Este algoritmo detecta correctamente a los tres perros presentes en la imagen, pero de las tres personas que hay, solo detecta a dos. Segmenta en distintas partes el paisaje de la imagen.

### Instance Segmentation con YoloV8 (Ultralytics)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install ultralytics==8.0.196
# 
# from IPython import display
# display.clear_output()
# 
# import ultralytics
# ultralytics.checks()

from ultralytics import YOLO
from PIL import Image, ImageDraw
import numpy as np
from IPython.display import display

# Cargar el modelo YOLOv8 con capacidad de segmentación
model = YOLO("yolov8m-seg.pt")

# Realizar predicciones en la imagen
results = model.predict('perros-plaza.jpg')

# Seleccionar el primer resultado (en caso de que haya varios)
result = results[0]

# Abrir la imagen original para dibujar sobre ella
img = Image.open("perros-plaza.jpg")
draw = ImageDraw.Draw(img)

# Para cada detección, dibujar el bounding box y mostrar la clase
for box in results[0].boxes:
    # Extracción de coordenadas del bounding box y clase
    x1, y1, x2, y2, conf, cls = box.data[0]
    label = result.names[int(cls)]  # Asumiendo que result.names contiene los nombres de las clases

    # Dibujar el bounding box
    draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 0), width=2)

    # Mostrar la clase (y opcionalmente la confianza) en el bounding box
    text = f"{label} {conf:.2f}"
    draw.text((x1, y1), text, fill=(0, 255, 0))

# Acceder a las máscaras del resultado
masks = result.masks

# Iterar a través de todas las máscaras encontradas
for mask in masks:
    # Convertir la máscara a un array numpy y obtener el polígono
    mask_data = mask.data[0].numpy()
    polygon = mask.xy[0]

    # Dibujar el polígono correspondiente a cada máscara en la imagen original
    polygon_list = [(p[0], p[1]) for p in polygon]
    draw.polygon(polygon_list, outline=(255, 0, 0), width=3)

# Mostrar la imagen con los polígonos dibujados
display(img)

"""Este algortimo detecta correctamente a las personas y perros presentes en la imagen. Dentro de los bounding boxes, segmenta en su mayoría bien a los objetos, siguiendo la silueta de los mismos.

###SAM (Segment Anything Model)
"""

import torch
import torchvision
print("PyTorch version:", torch.__version__)
print("Torchvision version:", torchvision.__version__)
print("CUDA is available:", torch.cuda.is_available())
import sys
!{sys.executable} -m pip install opencv-python matplotlib
!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'
!{sys.executable} -m pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision

!mkdir images
!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg

!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

# Función para mostrar la segmentación
def show_anns(anns):
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)

    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    for ann in sorted_anns:
        m = ann['segmentation']
        color_mask = np.concatenate([np.random.random(3), [0.35]])
        img[m] = color_mask
    ax.imshow(img)

image = cv2.imread('perros.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor

sam_checkpoint = "sam_vit_h_4b8939.pth"
model_type = "vit_h"

device = "cuda"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

mask_generator = SamAutomaticMaskGenerator(sam)

masks = mask_generator.generate(image)

print(len(masks))
print(masks[0].keys())

plt.figure(figsize=(20,20))
plt.imshow(image)
show_anns(masks)
plt.axis('off')
plt.show()
